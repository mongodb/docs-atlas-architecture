.. _arch-center-dr:

=================
Disaster Recovery
=================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 3
   :class: onecol

It is critical for enterprises to plan for disaster recovery. We strongly recommend that you prepare a comprehensive disaster recovery (DR) plan 
that includes elements such as: 

- Your designated recovery point objective (RPO)
- Your designated recovery time objective (RTO)
- Any automated processes that facilitate alignment with these objectives
- The recommendations on this page

Use the recommendations on this page to prepare for and respond to
disasters.

To learn more about evaluating a backup and DR strategy for |service|, see 
`Data Resilience with MongoDB <https://www.mongodb.com/resources/products/capabilities/data-resilience-strategy-with-mongodb-atlas>`__. 

{+service+} Features and Recommendations for Disaster Recovery
--------------------------------------------------------------

Features
~~~~~~~~

To learn about {+service+} features that support disaster recovery, see
the following pages in the {+atlas-arch-center+}:

- :ref:`arch-center-high-availability`
- :ref:`arch-center-resiliency`
- :ref:`arch-center-backups`

Proactive Configuration Recommendations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use the following proactive configuration recommendations to configure your
{+service+} deployments and backups for quicker recovery from disasters.

Members of the Same Replica Sets Should Not Share Resources
```````````````````````````````````````````````````````````

MongoDB provides high availability by having multiple copies of data in replica sets. Members of the same replica set should not share the same resources. For example, members of the same replica set should not
share the same physical hosts and disks. You can ensure that replica
sets do not share resources by :ref:`distributing data across data centers <arch-center-distribute-data>`.

Use an Odd Number of Replica Set Members
````````````````````````````````````````

To elect a :manual:`primary </core/replica-set-members>`, you need a majority of :manual:`voting </core/replica-set-elections>` replica set members available. We recommend that you create replica sets with an
odd number of voting replica set members. There is no benefit in having
an even number of voting replica set members.

Fault tolerance is the number of replica set members that can become
available and still leave enough members available for a primary. 
Fault tolerance of a four-member replica set is the same as for a three-member replica set because both can withstand a single-node
outage.

To learn more about replica set members, see :manual:`Replica Set Members </core/replica-set-members>`. To learn more about replica set elections and 
voting, see :manual:`Replica Set Elections </core/replica-set-elections>`.

.. _arch-center-distribute-data:

Distribute Data Across At Least Three Data Centers
``````````````````````````````````````````````````

To guarantee that a replica set can elect a primary if a data center
becomes unavailable, you must distribute nodes across at least three
data centers.

Consider the following diagram, which shows data distributed across
two data centers:

**DIAGRAM BELOW TO BE CHANGED FOR FINAL STYLIZED DIAGRAM**

.. figure:: /includes/images/two-data-centers.png
   :figwidth: 750px
   :alt: An image showing two data centers: Data Center 1, with a primary and a secondary node, and Data Center 2, with only a secondary node

In the previous diagram, if Data Center 2 becomes unavailable, a majority of replica set members remain available and {+service+} can elect a primary. However, if you lose Data Center 1, you have only one
out of three replica set members available, no majority, and the system degrades into read-only mode.

Consider the following diagram, which shows data distributed across
three data centers:

**DIAGRAM BELOW TO BE CHANGED FOR FINAL STYLIZED DIAGRAM**

.. figure:: /includes/images/three-data-centers.png
   :figwidth: 750px
   :alt: An image showing three data centers: Data Center 1, with a primary node, Data Center 2, with a secondary node, and Data Center 3, with a secondary node

When you distribute nodes across three data centers, if one data
center becomes unavailable, you still have two out of three replica set
members available, which maintains a majority to elect a primary.

You can distribute data across at least three data centers within the same region by choosing a region with at least three availability zones. Availability zones consist of one or more discrete data centers, each with redundant power, networking and connectivity, housed in separate facilities.

{+service+} uses availability zones for all cloud providers
automatically when you deploy a dedicated cluster to a region that supports availability zones. Atlas splits the cluster's nodes across availability zones. For example, for a three-node replica set {+cluster+} deployed to a three-availability-zone region, {+service+} deploys one node in each zone. A local failure in the data center hosting one node doesn't impact the operation of data centers hosting the other nodes.

We recommend that you deploy replica sets to the following regions because they support at least three availability zones:

**Recommended AWS Regions**

.. include:: /includes/aws-recommended-regions.rst

**Recommended Azure Regions**



**Recommended Google Cloud Regions**



Use ``mongos`` Redundancy for Sharded {+Clusters+}
`````````````````````

When a client connects to a sharded {+cluster+}, we recommend that you include multiple :manual:`mongos </reference/program/mongos/>`
processes in the connection URI. This allows
operations to route to different ``mongos`` instances for load
balancing, but it is also important for disaster recovery. 

Consider the following diagram, which shows a sharded {+cluster+}
spread across three data centers. The application connects to the {+cluster+} from a remote location. If Data Center 3 becomes unavailable, the application can still connect to the ``mongos``
processes in the other data centers.

**DIAGRAM BELOW TO BE CHANGED FOR FINAL STYLIZED DIAGRAM**

.. figure:: /includes/images/mongos-three-data-centers.png
   :figwidth: 750px
   :alt: An image showing three data centers: Data Center 1, with a primary shards and two mongos, Data Center 2, with secondary shards and two mongos, and Data Center 3, with secondary shards and two mongos. The application connects to all six mongos instances.

You can use 
:manual:`retryable reads </core/retryable-reads/>` and
:manual:`retryable writes</core/retryable-writes/>` to simplify the required error handling for the previous
configuration.

Use ``majority`` Write Concern
``````````````````````````````

MongoDB allows you to specify the level of acknowledgment requested
for write operations by using your :manual:`write concern 
</reference/write-concern/>`. For example, if
you had a three-node replica set and had a write concern of
``majority``, every write operation would need to be persisted on 
two nodes before an acknowledgment of completion sends to the driver
that issued said write operation. For protection during a regional
outage, we recommend that you set the write concern to ``majority``.

To understand the importance of ``majorty`` write concern, imagine a
five-node replica set spread across three separate regions with a 2-2-1
topology (two regions with two nodes and one region with one node),
with a write concern of ``4``. In the event that one of the regions
with two nodes becomes unavailable due to an outage and only three
nodes are available, no write operations complete and the
operation hangs due because it is unable to persist data on four nodes.
In this scenario, despite the availability of the majority of nodes in the replica set, the database
behaves the same as if a majority of nodes in the replica set were
unavailable. If you use ``majority`` write concern rather than a numeric value, it prevents this scenario.

Consider Backup Configuration
`````````````````````````````
Frequent data backups is critical for business continuity and disaster recovery. Frequent backups ensure that data loss and downtime is
minimal in the event of the loss of normal operations, whether from a disaster or cyber attack. 

We recommend that you:

- Set your backup frequency to meet your desired business continuity
  objectives. Continuous backups may be needed for some systems, while less frequent snapshots may be desirable for others.
- Store backups in a different physical location than the
  source data.
- Test your backup recovery process to ensure that you can restore
  backups in a repeatable and timely manner.
- Ensure that your {+clusters+} run the same MongoDB versions for
  compatibility during restore.
- Configure a :atlas:`backup compliance policy 
  </backup/cloud-backup/backup-compliance-policy/#std-label-backup-compliance-policy>` to prevent deleting backup
  snapshots, prevent decreasing the snapshot retention time, and more.

For more backup recommendations, see :ref:`arch-center-backups`.

Disaster Recovery Recommendations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use the following disaster recovery recommendations to create a {+DR+}
plan for your organization. These recommendations provide information
on steps to take in the event of a disaster.

It is imperative that you test the plans in this section regularly (ideally quarterly, but at least semi-annually). Testing often helps prepare the Enterprise Database Management (EDM) team for when disaster strikes while also helping to keep the instructions up to date. 

Your EDM team should test the following scenarios:

- A regional outage affecting the {+cluster+}
- A regional outage affecting the application
- Any disaster that requires restoring a backup

Some disaster recovery testing may require actions that cannot be performed by EDM users. In these cases, open a support case for the purpose of performing artificial outages at least a week in advance of when you plan on running a test exercise.

Single Node Outage
``````````````````

In the event that a single node in your replica set fails due to a regional outage, your deployment should still be available, assuming you have followed best practices. If you are reading from secondaries, you may experience degraded performance since you have one less node to read from.

You can test a primary node outage in {+service+} using the 
{+atlas-ui+}\'s
:atlas:`Test Primary Failover </tutorial/test-resilience/test-primary-failover/>` feature or the :oas-atlas-op:`Test Failover </testFailover>` {+atlas-admin-api+} endpoint.

Regional Outage
```````````````

If a single region outage or multi-region outage degrades the state of your {+cluster+}, follow these steps:

.. procedure::
   :style: normal

   .. step:: Identify the region experiencing issues

   .. step:: Identify how many nodes are still online

      You can find info about {+cluster+} health in the {+cluster+}\'s :guilabel:`Overview` tab of the
      {+atlas-ui+}.

   .. step:: Determine how many nodes you require
    
      Based on how many nodes are left online, determine how many new nodes you require to restore the replica set to a normal state.
    
      A normal state is a state in which the majority of nodes
      are available.

   .. step:: Determine which regions are unlikely to be affected by the current outage
    
      Depending on the cause of the outage, there may be additional regions in the near future that will also experience unscheduled outages. For example, if the outages were caused by a natural disaster on the east coast of the United States, you should avoid regions on the east coast of the United States in case there are additional issues.

   .. step:: Add nodes to the regions you identified
    
      Add the required number of nodes for a normal state across
      regions that are unlikely to be affected by the cause of the
      outage.

      To reconfigure a replica set during an outage by adding regions
      or nodes, see :atlas:`</reconfigure-replica-set-during-regional-outage/>`.

   .. step:: (Optional) Add additional nodes
    
      In addition to adding nodes to restore your replica set to a
      normal state, you can add additional nodes to match the topology of your replica set before the disaster.

You can test a region outage in {+service+} using the 
{+atlas-ui+}\'s
:atlas:`Simulate Outage </tutorial/test-resilience/simulate-regional-outage/>` feature or the :oas-atlas-op:`Start an Outage Simulation </startOutageSimulation>` {+atlas-admin-api+} endpoint.




