.. _arch-center-dr:

:noprevnext:

=================
Disaster Recovery
=================

.. default-domain:: mongodb

.. facet::
   :name: genre
   :values: tutorial

.. meta::
   :keywords: atlas architecture center, disaster recovery
   :description: Learn how to prepare for and respond to disasters with this disaster recovery page for MongoDB Atlas.

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: onecol

It is critical for enterprises to plan for disaster recovery. We strongly recommend that you prepare a comprehensive disaster recovery (DR) plan 
that includes elements such as: 

- Your designated recovery point objective (RPO)
- Your designated recovery time objective (RTO)
- Automated processes that facilitate alignment with these objectives

Use the recommendations on this page to prepare for and respond to
disasters.

{+service+} Features and Recommendations for Disaster Recovery
--------------------------------------------------------------

Features
~~~~~~~~

To learn about {+service+} features that support disaster recovery, see
the following pages in the {+atlas-arch-center+}:

- :ref:`arch-center-high-availability`
- :ref:`arch-center-resiliency`
- :ref:`arch-center-backups`

Proactive Configuration Recommendations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use the following proactive configuration recommendations to configure your
{+service+} deployments and backups to expediate recovery from disasters.

Members of the Same Replica Sets Should Not Share Resources
```````````````````````````````````````````````````````````

MongoDB provides high availability by having multiple copies of data in replica sets. Members of the same replica set should not share the same resources. For example, members of the same replica set should not
share the same physical hosts and disks. You can ensure that replica
sets don't share resources by :ref:`distributing data across data centers <arch-center-distribute-data>`.

Use an Odd Number of Replica Set Members
````````````````````````````````````````

To elect a :manual:`primary </core/replica-set-members>`, you need a majority of :manual:`voting </core/replica-set-elections>` replica set members available. We recommend that you create replica sets with an
odd number of voting replica set members. There is no benefit in having
an even number of voting replica set members.

Fault tolerance is the number of replica set members that can become
unavailable with enough members still available for a primary election. 
Fault tolerance of a four-member replica set is the same as for a three-member replica set because both can withstand a single-node
outage.

To learn more about replica set members, see :manual:`Replica Set Members </core/replica-set-members>`. To learn more about replica set elections and 
voting, see :manual:`Replica Set Elections </core/replica-set-elections>`.

.. _arch-center-distribute-data:

Distribute Data Across At Least Three Data Centers
``````````````````````````````````````````````````

To guarantee that a replica set can elect a primary if a data center
becomes unavailable, you must distribute nodes across at least three
data centers.

Consider the following diagram, which shows data distributed across
two data centers:

**DIAGRAM BELOW TO BE CHANGED FOR FINAL STYLIZED DIAGRAM**

.. figure:: /includes/images/two-data-centers.png
   :figwidth: 750px
   :alt: An image showing two data centers: Data Center 1, with a primary and a secondary node, and Data Center 2, with only a secondary node

In the previous diagram, if Data Center 2 becomes unavailable, a majority of replica set members remain available and {+service+} can elect a primary. However, if you lose Data Center 1, you have only one
out of three replica set members available, no majority, and the system degrades into read-only mode.

Consider the following diagram, which shows data distributed across
three data centers:

**DIAGRAM BELOW TO BE CHANGED FOR FINAL STYLIZED DIAGRAM**

.. figure:: /includes/images/three-data-centers.png
   :figwidth: 750px
   :alt: An image showing three data centers: Data Center 1, with a primary node, Data Center 2, with a secondary node, and Data Center 3, with a secondary node

When you distribute nodes across three data centers, if one data
center becomes unavailable, you still have two out of three replica set
members available, which maintains a majority to elect a primary.

You can distribute data across at least three data centers within the same region by choosing a region with at least three availability zones. Availability zones consist of one or more discrete data centers, each with redundant power, networking and connectivity, housed in separate facilities.

{+service+} uses availability zones for all cloud providers
automatically when you deploy a dedicated cluster to a region that supports availability zones. Atlas splits the cluster's nodes across availability zones. For example, for a three-node replica set {+cluster+} deployed to a three-availability-zone region, {+service+} deploys one node in each zone. A local failure in the data center hosting one node doesn't impact the operation of data centers hosting the other nodes.

We recommend that you deploy replica sets to the following regions because they support at least three availability zones:

Recommended AWS Regions
#######################

.. include:: /includes/aws-recommended-regions.rst

Recommended Azure Regions
#########################

.. include:: /includes/azure-recommended-regions.rst

Recommended Google Cloud Regions
################################

.. include:: /includes/gcp-recommended-regions.rst

Use ``mongos`` Redundancy for Sharded {+Clusters+}
```````````````````````````````````````````````````

When a client connects to a sharded {+cluster+}, we recommend that you include multiple :manual:`mongos </reference/program/mongos/>`
processes in the connection URI. This allows
operations to route to different ``mongos`` instances for load
balancing, but it is also important for disaster recovery. 

Consider the following diagram, which shows a sharded {+cluster+}
spread across three data centers. The application connects to the {+cluster+} from a remote location. If Data Center 3 becomes unavailable, the application can still connect to the ``mongos``
processes in the other data centers.

**DIAGRAM BELOW TO BE CHANGED FOR FINAL STYLIZED DIAGRAM**

.. figure:: /includes/images/mongos-three-data-centers.png
   :figwidth: 750px
   :alt: An image showing three data centers: Data Center 1, with a primary shards and two mongos, Data Center 2, with secondary shards and two mongos, and Data Center 3, with secondary shards and two mongos. The application connects to all six mongos instances.

You can use 
:manual:`retryable reads </core/retryable-reads/>` and
:manual:`retryable writes</core/retryable-writes/>` to simplify the required error handling for the previous
configuration.

Use ``majority`` Write Concern
``````````````````````````````

MongoDB allows you to specify the level of acknowledgment requested
for write operations by using :manual:`write concern 
</reference/write-concern/>`. For example, if
you had a three-node replica set and had a write concern of
``majority``, every write operation would need to be persisted on 
two nodes before an acknowledgment of completion sends to the driver
that issued said write operation. For protection during a regional
outage, we recommend that you set the write concern to ``majority``.

To understand the importance of ``majorty`` write concern, imagine a
five-node replica set spread across three separate regions with a 2-2-1
topology (two regions with two nodes and one region with one node),
with a write concern of ``4``. If one of the regions
with two nodes becomes unavailable due to an outage and only three
nodes are available, no write operations complete and the
operation hangs because it is unable to persist data on four nodes.
In this scenario, despite the availability of the majority of nodes in the replica set, the database
behaves the same as if a majority of the nodes in the replica set were
unavailable. If you use ``majority`` write concern rather than a numeric value, it prevents this scenario.

Consider Backup Configuration
`````````````````````````````
Frequent data backups is critical for business continuity and disaster recovery. Frequent backups ensure that data loss and downtime is
minimal if a disaster or cyber attack disrupts normal operations. 

We recommend that you:

- Set your backup frequency to meet your desired business continuity
  objectives. Continuous backups may be needed for some systems, while less frequent snapshots may be desirable for others.
- Store backups in a different physical location than the
  source data.
- Test your backup recovery process to ensure that you can restore
  backups in a repeatable and timely manner.
- Ensure that your {+clusters+} run the same MongoDB versions for
  compatibility during restore.
- Configure a :atlas:`backup compliance policy 
  </backup/cloud-backup/backup-compliance-policy/#std-label-backup-compliance-policy>` to prevent deleting backup
  snapshots, prevent decreasing the snapshot retention time, and more.

For more backup recommendations, see :ref:`arch-center-backups`.

Plan Your Resource Utilization
``````````````````````````````

To avoid resource capacity issues, we recommend that you monitor
resource utilization and hold regular capacity planning sessions.
MongoDB Professional Services offers these sessions.

To view your resource utilization, see :atlas:`Monitor Real-Time Performance </real-time-performance-panel>`. To view metrics with the {+atlas-admin-api+}, see :oas-atlas-tag:`Monitoring and Logs </Monitoring-and-Logs>`.

To learn best practices for alerts and monitoring for resource
utilization, see :ref:`arch-center-monitoring-alerts`.

If you encounter resource capacity issues, see :ref:`arch-center-resource-capacity`.

Plan Your MongoDB Version Changes
`````````````````````````````````

Ensure that you perform MongoDB major version upgrades far before 
your current version reaches `end of life <https://www.mongodb.com/legal/support-policy/lifecycles>`__. 

You can't downgrade your MongoDB version using the {+atlas-ui+}, so we recommend that you work directly with MongoDB Professional or Technical
Services when planning and executing a major version upgrade to avoid any issues that might occur during the upgrade process.

Disaster Recovery Recommendations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use the following disaster recovery recommendations to create a {+DR+}
plan for your organization. These recommendations provide information
on steps to take in the event of a disaster.

It is imperative that you test the plans in this section regularly (ideally quarterly, but at least semi-annually). Testing often helps prepare the Enterprise Database Management (EDM) team to respond to disasters while also helping to keep the instructions up to date. 

Some disaster recovery testing might require actions that cannot be performed by EDM users. In these cases, open a support case for the purpose of performing artificial outages at least a week in advance of when you plan on running a test exercise.

This section covers the following disaster recovery procedures:

- :ref:`arch-center-single-node-outage`
- :ref:`arch-center-regional-outage`
- :ref:`arch-center-provider-outage`
- :ref:`arch-center-atlas-outage`
- :ref:`arch-center-resource-capacity`
- :ref:`arch-center-resource-failure`
- :ref:`arch-center-data-deletion`
- :ref:`arch-center-driver-failure`
- :ref:`arch-center-data-corruption`

.. _arch-center-single-node-outage:

Single Node Outage
``````````````````

If a single node in your replica set fails due to a regional outage, your deployment should still be available, assuming you have followed best practices. If you are reading from secondaries, you might experience degraded performance because you have one less node to read from.

You can test a primary node outage in {+service+} using the 
{+atlas-ui+}\'s
:atlas:`Test Primary Failover </tutorial/test-resilience/test-primary-failover/>` feature or the :oas-atlas-op:`Test Failover </testFailover>` {+atlas-admin-api+} endpoint.

.. _arch-center-regional-outage:

Regional Outage
```````````````

If a single region outage or multi-region outage degrades the state of your {+cluster+}, follow these steps:

.. procedure::
   :style: normal

   .. step:: Identify the region experiencing issues

   .. step:: Identify how many nodes are still online

      You can find info about {+cluster+} health in the {+cluster+}\'s :guilabel:`Overview` tab of the
      {+atlas-ui+}.

   .. step:: Determine how many nodes you require
    
      Based on how many nodes are left online, determine how many new nodes you require to restore the replica set to a normal state.
    
      A normal state is a state in which the majority of nodes
      are available.

   .. step:: Determine which regions are unlikely to be affected by the current outage
    
      Depending on the cause of the outage, there may be additional regions in the near future that will also experience unscheduled outages. For example, if the outages were caused by a natural disaster on the east coast of the United States, you should avoid regions on the east coast of the United States in case there are additional issues.

   .. step:: Add nodes to the regions you identified
    
      Add the required number of nodes for a normal state across
      regions that are unlikely to be affected by the cause of the
      outage.

      To reconfigure a replica set during an outage by adding regions
      or nodes, see 
      :atlas:`Reconfigure a Replica Set During a Regional Outage </reconfigure-replica-set-during-regional-outage/>`.

   .. step:: (Optional) Add additional nodes
    
      In addition to adding nodes to restore your replica set to a
      normal state, you can add additional nodes to match the topology of your replica set before the disaster.

You can test a region outage in {+service+} using the 
{+atlas-ui+}\'s
:atlas:`Simulate Outage </tutorial/test-resilience/simulate-regional-outage/>` feature or the :oas-atlas-op:`Start an Outage Simulation </startOutageSimulation>` {+atlas-admin-api+} endpoint.

.. _arch-center-provider-outage:

Cloud Provider Outage
``````````````````````

In the highly unlikely event that an entire cloud provider is
unavailable, follow these steps to bring your deployment back online:

.. procedure::
   :style: normal
   
   .. step:: Determine when the cloud provider outage began

   .. step:: Identify the alternative cloud provider you would like to deploy your new {+cluster+} on

      For a list of cloud providers and information, see 
      :atlas:`Cloud Providers </reference/cloud-providers>`.

   .. step:: Find the most recent available snapshot taken of the {+cluster+} before the outage began

      To learn how to view your backup snapshots, see 
      :atlas:`View M10+ Backup Snapshots </backup/cloud-backup/dedicated-cluster-backup/#view-m10--backup-snapshots>`.

   .. step:: Create a new {+cluster+} with the alternative cloud provider

      Your new {+cluster+} must have an identical topology of the original cluster.

   .. step:: Restore the most recent snapshot from the previous step into the new {+cluster+}

      To learn how to restore your snapshot, see :atlas:`Restore Your Cluster </backup/cloud-backup/restore-overview/>`.

   .. step:: Switch any applications that connect to the old {+cluster+} to the newly-created {+cluster+}

      To find the new connection string, see :atlas:`Connect via Drivers </driver-connection>`.

.. _arch-center-atlas-outage:

{+service+} Outage
``````````````````

In the highly unlikely event that the {+service+} Control Plane is
unavailable, open a high-priority :atlas:`support ticket </support/#request-support>`. 

Your {+cluster+}
might still be available and accessible even if the {+atlas-ui+} is
unavailable.

.. _arch-center-resource-capacity:

Resource Capacity Issues
````````````````````````

Computational resource (such as disk space, RAM, or CPU) capacity
issues can result from poor planning or unexpected database traffic.
This behavior might not be a result of a disaster. 

If a computational resource reaches the maximum allocated
amount and causes a disaster, follow these steps:

.. procedure::
   :style: normal

   .. step:: Identify which computational resource is maxed out by using the Real Time Performance Panel or {+service+} metrics

      To view your resource utilization in the {+atlas-ui+}, see :atlas:`Monitor Real-Time Performance </real-time-performance-panel>`.

      To view metrics with the {+atlas-admin-api+},
      see :oas-atlas-tag:`Monitoring and Logs </Monitoring-and-Logs>`.

   .. step:: Determine how much more of the maxed-out resource you need to alleviate performance issues

   .. step:: Allocate the necessary resources
      
      Note that Atlas will perform this change in a rolling manner, so
      it should not have any major impact on your applications.

      To learn how to allocate more resources, see :atlas:`Edit a Cluster </scale-cluster/#edit-a-cluster>`.

   .. step:: Monitor your {+cluster+} to determine whether any other issues exist after the change

.. _arch-center-resource-failure:

Resource Failure
````````````````

.. include:: /includes/temporary-cluster-fix.rst

If a computational resource fails and causes your cluster to become
unavailable, follow these steps:

.. procedure::
   :style: normal

   .. step:: Open a high-priority :atlas:`support ticket </support/#request-support>`

   .. step:: Create a new {+cluster+} with the same topology as the failing {+cluster+}

   .. step:: Restore the most recent backup into the newly-created {+cluster+}

      To learn how to restore your snapshot, see :atlas:`Restore Your Cluster </backup/cloud-backup/restore-overview/>`.

   .. step:: Point all applications using the failing {+cluster+} to the newly-created {+cluster+}

.. _arch-center-data-deletion:

Deletion of Production Data
```````````````````````````

Production data might be accidentally deleted due to human error or a bug
in the application built on top of the database.

If the contents of a collection or database have been deleted, follow these steps to restore your data:

.. procedure::
   :style: normal

   .. step:: Determine the date and time or oplog timestamp when the data was deleted

   .. step:: Create a copy of the current state of the collection or database, if it contains any data

      You can use `mongoexport <https://www.mongodb.com/docs/database-tools/mongoexport/>`__ to create a copy.

   .. step:: Restore your data
   
      If the deletion occurred within the last 72 hours, use Point in Time (PIT) restore to restore from the point in time right before the deletion occurred.

      If the deletion did not occur in the past 72 hours, restore the most recent backup from before the deletion occurred into the cluster.

      To learn more, see :atlas:`Restore Your Cluster </backup/cloud-backup/restore-overview/>`.

   .. step:: If you created a copy of your data, import the new data you exported

      You can use `mongoimport <https://www.mongodb.com/docs/database-tools/mongoimport/>`__ with `upsert mode <https://www.mongodb.com/docs/database-tools/mongoimport/#std-option-mongoimport.--mode>`__ to import your data and ensure that any data that was modified or added is reflected properly in the collection or database.

.. _arch-center-driver-failure:

Driver Failure
``````````````

If a driver fails, follow these steps:

.. procedure::
   :style: normal

   .. step:: Determine the issue

      You can work with the technical support team during this step.

      If you determine that reverting to an earlier driver version will
      fix the issue, continue to the next step.

   .. step:: Determine when the issue was introduced and the latest working driver version

   .. step:: Evaluate if any other changes are required to move to an earlier driver version
      
      This might include application code or query changes.

   .. step:: Test the changes in a non-production environment

   .. step:: If you don't encounter issues while testing, downgrade to the new driver version
      
      Ensure that any other changes from the previous step are
      reflected in the production environment.

.. _arch-center-data-corruption:

Data Corruption
```````````````

.. include:: /includes/temporary-cluster-fix.rst

If your underlying data becomes corrupted, follow these steps:

.. procedure::
   :style: normal

   .. step:: Open a high-priority :atlas:`support ticket </support/#request-support>`

   .. step:: Create a new {+cluster+} with the same topology as the failing {+cluster+}

   .. step:: Restore the most recent backup into the newly-created {+cluster+}

      To learn how to restore your snapshot, see :atlas:`Restore Your Cluster </backup/cloud-backup/restore-overview/>`.

   .. step:: Check the restored data to confirm that the corruption does not exist

   .. step:: Point all applications using the failing {+cluster+} to the newly-created {+cluster+}









