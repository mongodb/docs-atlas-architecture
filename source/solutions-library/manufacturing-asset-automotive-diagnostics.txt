.. _arch-center-is-automotive-diagnostics:

================================================
Automotive Diagnostics Using Atlas Vector Search
================================================

.. facet::
   :name: genre
   :values: tutorial

.. meta:: 
   :keywords: gen AI, vector search
   :description: Learn how to perform real-time analysis, advanced root cause diagnostics, and proactive maintenance using MongoDB Atlas Vector Search and AWS Bedrock.

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

Leverage MongoDB Atlas Vector Search and AWS Bedrock for advanced root
cause diagnostics, integrating diverse data types for real-time analysis
and proactive maintenance.

**Use cases:** `Gen AI
<https://www.mongodb.com/use-cases/artificial-intelligence>`__

**Industries:** `Manufacturing and Mobility
<https://www.mongodb.com/solutions/industries/manufacturing>`__,
Aerospace and Defense

**Products:** `MongoDB Atlas <http://mongodb.com/atlas>`__, `Atlas
Vector Search
<https://www.mongodb.com/products/platform/atlas-vector-search>`__

**Partners:** `Amazon Bedrock
<https://cloud.mongodb.com/ecosystem/amazon-bedrock>`__

Solution Overview
-----------------

The manufacturing industry is supported by a complex value chain that
spans from inventory management to connected equipment and products. The
key to solving problems, improving processes, and boosting the overall
efficiency and quality of this value chain is root-cause diagnostics.
Unlike predictive maintenance, which focuses on symptom management,
root-cause diagnostics digs deeper to identify the underlying sources of
issues, ensuring they are fixed permanently and do not recur.

Root-cause diagnostics offers several benefits:

- **Eliminates recurring problems:** By addressing the true root cause,
  we avoid temporary fixes and prevent the same issue from recurring,
  saving time, money, and resources.

- **Enhance process efficiency:** Identifying bottlenecks and
  inefficiencies at their source leads to increased output and lower
  production costs.

- **Promotes safety and environmental practices:**  Proactive
  interventions and risk mitigation foster safer and more environmentally
  friendly operations.

- **Drives continuous improvement:** The systematic approach of
  root-cause diagnostics encourages ongoing process improvement and
  innovation.

However, implementing root-cause diagnostics in manufacturing can be
challenging. The vast amounts of complex and noisy data from sensors and
machines, along with the need to integrate diverse data types, make it a
formidable task. Traditional methods rely heavily on human expertise,
knowledge, and experience.

Our solution explores the application of `AI
<https://www.mongodb.com/resources/basics/what-is-artificial-intelligence>`__
and `MongoDB Atlas Vector Search
<https://www.mongodb.com/products/platform/atlas-vector-search>`__ for
advanced root-cause diagnostics using sound input and the integration of
AWS Bedrock for real-time report generation on detected anomalies,
enhancing real-time monitoring and maintenance.

Reference Architectures
-----------------------

**With MongoDB**:

The demo architecture comprises several key components working together
to capture, store, analyze, and report data.


1. **Engine and Raspberry Pi**

   - **Engine control:**  A Raspberry Pi is connected to the engine,
     controlling it via a relay.
   - **Telemetry sensors:** The Raspberry Pi is equipped with sensors to
     measure telemetry data such as temperature and humidity.

2. **Car digital twin and mobile app**

   - **Virtual and physical integration:** A car digital twin in
     JavaScript  and an iPhone app are connected to the setup. Commands
     from the apps are sent to MongoDB, which streams them to the
     Raspberry Pi, triggering the relay to start both the physical engine
     and the digital twin.

3. **Audio diagnostics**

   - **Audio recording:** Every second, the engine’s audio is recorded.
   - **Vector conversion:** The audio clips are converted into vectors
     through an embedder and stored in MongoDB.
   - **Vector search:** Using `Atlas Vector Search
     <https://www.mongodb.com/products/platform/atlas-vector-search>`__,
     the system predicts the engine's status (off, running normally,
     detecting a metallic or soft hit). This information is displayed on
     the apps, providing real-time diagnostics.

4. **AWS Bedrock integration**

   - **Automated reporting:** When an anomaly is detected (e.g.,
     abnormal audio), Atlas triggers a function to send telemetry data and
     sound analysis results to AWS Bedrock.
   - **Report generation:** AWS Bedrock generates a detailed report
     based on the findings, which is then sent back to the dashboard for
     review.

This architecture creates a feedback loop where edge devices generate
data for real-time control and monitoring, now enhanced with audio
diagnostics through vectors. The integration showcases the power of
utilizing `Atlas Vector Search
<https://www.mongodb.com/products/platform/atlas-vector-search>`__ for
root-cause diagnostics, significantly improving efficiency, reliability,
and innovation in manufacturing operations.

.. figure:: /includes/images/industry-solutions/automotive-diagnostics-architecture.svg
   :figwidth: 1200px
   :alt: demo architecture for automotive diagnostics

   Figure 1. Demo architecture.

Building the Solution
---------------------

.. procedure::
   :style: normal

   .. step:: **Include meaningful CSV data for your use case**

      The data should be relevant to your use case and respect the
      following guidelines:

      - The CSV file should contain a header row with column names.
      - The first column must be named “timestamp” and contain
        timestamps in the format YYYY-MM-DDTHH:MM:SSZ, e.g.
        2025-02-19T13:00:00Z.
      - The remaining columns should contain relevant data for your use
        case. 
      - There is no column nor row limit, but for framework
        testing purposes the suggestion is to keep the data size as
        small as possible.

      Sample data:

      .. code-block:: none

         timestamp,gdp,interest_rate,unemployment_rate,vix
         2025-02-19T13:00:00Z,2.5,1.75,3.8,15
         2025-02-19T13:05:00Z,2.7,1.80,3.7,18
         2025-02-19T13:10:00Z,2.6,1.85,3.9,22
         2025-02-19T13:15:00Z,2.4,1.70,4.0,10
         2025-02-19T13:20:00Z,2.3,1.65,4.1,20

      Within the same folder you need to add a queries file. This file
      will contain the queries that will be used to showcase vector
      search capabilities as part of the agentic workflow. The file
      should respect the following guidelines:

      - The CSV file should contain a header row with column names. 
      - The first column must be named “query” and contain the queries. 
      - The second column must be named “recommendation” and contain the
        expected recommendations. 
      - There is no column or row limit, but for framework testing
        purposes the suggestion is to keep the data size as small as
        possible.

      Sample data:

      .. code-block:: none

         query,recommendation
         GDP growth slowing,Consider increasing bond assets to mitigate risks from potential economic slowdown.
         GDP showing strong growth,Increase equity assets to capitalize on favorable investment conditions.
         Interest rates rising,Shift focus to bond assets as higher rates may impact borrowing-sensitive sectors.
         Interest rates falling,Increase real estate assets to take advantage of lower borrowing costs.
         Unemployment rate increasing,Reduce equity assets to account for potential economic weakness and reduced consumer spending.
         Unemployment rate decreasing,Increase equity assets to benefit from improved economic conditions and corporate profits.
         VIX above 20,Reduce equity assets to manage risks associated with high market volatility.
         VIX below 12,Increase equity assets to capitalize on low market volatility and investor confidence.
         VIX within normal range (12-20),Maintain current asset allocation as market conditions are stable.
         Combination of rising interest rates and high VIX,Focus on bond assets to hedge against market volatility and borrowing cost impacts.

   .. step:: **Set up MongoDB database and collections**

      Log in to MongoDB Atlas and create a database named
      “agentic_<YOUR_USE_CASE>,” e.g. “agentic_macro_indicators”. Ensure
      the name is reflected in the environment variables. 
      
      Create the following collections:

      - Agent_profiles (for storing agent profiles): You can import some
        sample data to this collection using this file.
      - Queries (for storing queries): You must import the queries from
        the queries.csv file created in Step 1.

   .. step:: **Configure your agentic worklfow through a JSON config file**

      Go to the config folder and create or update the JSON config.json
      file. The file should contain the following structure:

      .. code-block:: json

         {  
            "CSV_DATA": "data/csv/<YOUR_FILE_NAME>.csv",  
            "MDB_DATABASE_NAME": "<YOUR_MONGODB_DATABASE_NAME>",  
            "MDB_TIMESERIES_COLLECTION": "<YOUR_MONGODB_TIMESERIES_COLLECTION_NAME>",  
            "DEFAULT_TIMESERIES_DATA": [  
               {  
                  "timestamp": "<DEFAULT_TIMESTAMP_IN_YYYY-MM-DDTHH:MM:SSZ>"  
                  // Your default data here, check config_example.json for better understanding  
               }  
            ],  
            "CRITICAL_CONDITIONS": {  
               // Below is an example of a critical condition for GDP growth  
               "gdp": {"threshold": 2.5, "condition": "<", "message": "GDP growth slowing: {value}%"}  
               // Other critical conditions for your use case here, check config_example.json for better understanding  
            },  
            "MDB_TIMESERIES_TIMEFIELD": "<YOUR_TIMESTAMP_FIELD_NAME>",  
            "MDB_TIMESERIES_GRANULARITY": "<YOUR_TIMESERIES_GRANULARITY>",  
            "MDB_EMBEDDINGS_COLLECTION": "queries", // Using "queries" collection name for storing queries  
            "MDB_EMBEDDINGS_COLLECTION_VS_FIELD": "query_embedding", // Using "query_embedding" field for storing embeddings  
            "MDB_VS_INDEX": "<YOUR_MONGODB_DATABASE_NAME>_queries_vs_idx", // Replace <YOUR_MONGODB_DATABASE_NAME> with your MongoDB database name  
            "MDB_HISTORICAL_RECOMMENDATIONS_COLLECTION": "historical_recommendations", // Using "historical_recommendations" collection name for storing recommendations  
            "SIMILAR_QUERIES": [  
               // Below is an example of default similar queries for GDP growth  
               {  
                  "query": "GDP growth slowing",  
                  "recommendation": "Consider increasing bond assets to mitigate risks from potential economic slowdown."  
               }  
               // Other similar queries for your use case here, check config_example.json for better understanding  
               // This ones are going to be used for the vector search tool in case something is not found in the queries collection  
            ],  
            "MDB_CHAT_HISTORY_COLLECTION": "chat_history", // Using "chat_history" collection name for storing chat history  
            "MDB_CHECKPOINTER_COLLECTION": "checkpoints", // Using "checkpoints" collection name for storing checkpoints  
            "MDB_LOGS_COLLECTION": "logs", // Using "logs" collection name for storing logs  
            "MDB_AGENT_PROFILES_COLLECTION": "agent_profiles", // Using "agent_profiles" collection name for storing agent profiles  
            "MDB_AGENT_SESSIONS_COLLECTION": "agent_sessions", // Using "agent_sessions" collection name for storing agent sessions  
            "AGENT_PROFILE_CHOSEN_ID": "<YOUR_AGENT_PROFILE_ID>", // Replace <YOUR_AGENT_PROFILE_ID> with the agent profile ID you want to use, check config_example.json for better understanding  
            // Below is an example default agent profile for Portfolio Advisor  
            "DEFAULT_AGENT_PROFILE": {  
               "agent_id": "DEFAULT",  
               "profile": "Default Agent Profile",  
               "role": "Expert Advisor",  
               "kind_of_data": "Specific Data",  
               "motive": "diagnose the query and provide recommendations",  
               "instructions": "Follow procedures meticulously.",  
               "rules": "Document all steps.",  
               "goals": "Provide actionable recommendations."  
            },  
            "EMBEDDINGS_MODEL_NAME": "Cohere Embed English V3 Model (within AWS Bedrock)", // Describing the embeddings model used for creating the chain of thought  
            "EMBEDDINGS_MODEL_ID": "cohere.embed-english-v3", // Model ID for the embeddings model  
            "CHATCOMPLETIONS_MODEL_NAME": "Anthropic Claude 3 Haiku (within AWS Bedrock)", // Describing the chat completions model used for generating responses  
            "CHATCOMPLETIONS_MODEL_ID": "anthropic.claude-3-haiku-20240307-v1:0", // Model ID for the chat completions model  
            // Below is a sample agent workflow graph that uses the tools defined in the agent_tools.py file  
            // PLEASE BE CAREFUL WHEN MODIFYING THIS GRAPH, CONSIDER THAT THE TOOLS DEFINED IN THE AGENT TOOLS FILE ARE USED HERE AS WELL AS THE IMPORTS  
            "AGENT_WORKFLOW_GRAPH": {  
               "nodes": [  
                  {"id": "reasoning_node", "tool": "agent_tools.generate_chain_of_thought_tool"},  
                  {"id": "data_from_csv", "tool": "agent_tools.get_data_from_csv_tool"},  
                  {"id": "process_data", "tool": "agent_tools.process_data_tool"},  
                  {"id": "embedding_node", "tool": "agent_tools.get_query_embedding_tool"},  
                  {"id": "vector_search", "tool": "agent_tools.vector_search_tool"},  
                  {"id": "process_vector_search", "tool": "agent_tools.process_vector_search_tool"},  
                  {"id": "persistence_node", "tool": "agent_tools.persist_data_tool"},  
                  {"id": "recommendation_node", "tool": "agent_tools.get_llm_recommendation_tool"}  
               ],  
               "edges": [  
                  {"from": "reasoning_node", "to": "data_from_csv"},  
                  {"from": "data_from_csv", "to": "process_data"},  
                  {"from": "process_data", "to": "embedding_node"},  
                  {"from": "embedding_node", "to": "vector_search"},  
                  {"from": "vector_search", "to": "process_vector_search"},  
                  {"from": "process_vector_search", "to": "persistence_node"},  
                  {"from": "persistence_node", "to": "recommendation_node"},  
                  {"from": "recommendation_node", "to": "END"}  
               ],  
               "entry_point": "reasoning_node"  
            }  
         }  

      Once the config file is updated, environment variables need to be
      configured and vector embeddings created. Finally the vector
      search index must be created.

      Check out the `readme
      <https://github.com/mongodb-industry-solutions/agentic-framework/tree/a14ed55e9b4a5628da1798b5e4bbc59d695d43a7?tab=readme-ov-file#step-4-configure-your-agentic-workflow-through-a-json-config-file>`__
      for detailed explanations of all the steps.

      .. figure:: /includes/images/industry-solutions/framework-rapid-logical-architecture.svg
         :figwidth: 1200px
         :alt: Real-time wind turbine diagnosis

         Figure 3. Logical architecture.

Key Learnings
-------------

Agentic AI ushers in an entirely new type of software—one with the
ability to dramatically alter the way we process data to drive richer
and more efficient business data processing outcomes. In order to
leverage agents within business applications, software delivery teams
will need to learn:

- How agents get instructions for reacting within predetermined workflows.
- How agents access and interact with tooling like APIs and databases.
- How agents persist and manage state.

Many of these items can be considered baseline activities and setup for
software delivery teams, regardless of the actual use case, workflow, or
industry context for which an agent, or agentic workflow, will be
deployed.

For high performing software delivery teams, the use of a framework to
abstract common tasks and components has often been employed to speed up
the development and deployment of business software solutions, as well
as decrease the subsequent complexity of maintaining them. This agentic
framework makes it easier for software delivery teams to get started
with building agentic solutions, as well as maintain them, via common,
configurable components and assets.

Working in this framework can not only teach you and your software teams
how to get started with building agentic solutions, it can also help
them build repeatable patterns and components that make it easier to
scale and maintain your business solutions for the long term.

Authors
-------

- Julian Boronat, MongoDB
- Peyman Parsi, MongoDB
- Jeff Needham, MongoDB
- Luca Napoli, MongoDB
- Humza Akthar, MongoDB