.. _arch-center-is-automotive-diagnostics:

================================================
Automotive Diagnostics Using Atlas Vector Search
================================================

.. facet::
   :name: genre
   :values: tutorial

.. meta:: 
   :keywords: gen AI, vector search
   :description: Learn how to perform real-time analysis, advanced root cause diagnostics, and proactive maintenance using MongoDB Atlas Vector Search and AWS Bedrock.

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

Leverage MongoDB Atlas Vector Search and AWS Bedrock for advanced root
cause diagnostics, integrating diverse data types for real-time analysis
and proactive maintenance.

**Use cases:** `Gen AI
<https://www.mongodb.com/use-cases/artificial-intelligence>`__

**Industries:** `Manufacturing and Mobility
<https://www.mongodb.com/solutions/industries/manufacturing>`__,
Aerospace and Defense

**Products:** `MongoDB Atlas <http://mongodb.com/atlas>`__, `Atlas
Vector Search
<https://www.mongodb.com/products/platform/atlas-vector-search>`__

**Partners:** `Amazon Bedrock
<https://cloud.mongodb.com/ecosystem/amazon-bedrock>`__

Solution Overview
-----------------

The manufacturing industry is supported by a complex value chain that
spans from inventory management to connected equipment and products. The
key to solving problems, improving processes, and boosting the overall
efficiency and quality of this value chain is root-cause diagnostics.
Unlike predictive maintenance, which focuses on symptom management,
root-cause diagnostics digs deeper to identify the underlying sources of
issues, ensuring they are fixed permanently and do not recur.

Root-cause diagnostics offers several benefits:

- **Eliminates recurring problems:** By addressing the true root cause,
  we avoid temporary fixes and prevent the same issue from recurring,
  saving time, money, and resources.

- **Enhance process efficiency:** Identifying bottlenecks and
  inefficiencies at their source leads to increased output and lower
  production costs.

- **Promotes safety and environmental practices:**  Proactive
  interventions and risk mitigation foster safer and more environmentally
  friendly operations.

- **Drives continuous improvement:** The systematic approach of
  root-cause diagnostics encourages ongoing process improvement and
  innovation.

However, implementing root-cause diagnostics in manufacturing can be
challenging. The vast amounts of complex and noisy data from sensors and
machines, along with the need to integrate diverse data types, make it a
formidable task. Traditional methods rely heavily on human expertise,
knowledge, and experience.

Our solution explores the application of `AI
<https://www.mongodb.com/resources/basics/what-is-artificial-intelligence>`__
and `MongoDB Atlas Vector Search
<https://www.mongodb.com/products/platform/atlas-vector-search>`__ for
advanced root-cause diagnostics using sound input and the integration of
AWS Bedrock for real-time report generation on detected anomalies,
enhancing real-time monitoring and maintenance.

Reference Architectures
-----------------------

**With MongoDB**:

The demo architecture comprises several key components working together
to capture, store, analyze, and report data.


1. **Engine and Raspberry Pi**

   - **Engine control:**  A Raspberry Pi is connected to the engine,
     controlling it via a relay.
   - **Telemetry sensors:** The Raspberry Pi is equipped with sensors to
     measure telemetry data such as temperature and humidity.

2. **Car digital twin and mobile app**

   - **Virtual and physical integration:** A car digital twin in
     JavaScript  and an iPhone app are connected to the setup. Commands
     from the apps are sent to MongoDB, which streams them to the
     Raspberry Pi, triggering the relay to start both the physical engine
     and the digital twin.


.. figure:: /includes/images/industry-solutions/framework-rapid-reference-architecture.svg
   :figwidth: 1200px
   :alt: Real-time wind turbine diagnosis

   Figure 2. Agentic AI reference architecture.

Why MongoDB
-----------

MongoDB plays a pivotal role in enabling agentic AI. MongoDB's unique
combination of flexibility, performance, and scalability makes it an
ideal choice for enabling artificial intelligence. The MongoDB document
model enables native storage and retrieval of structured,
semi-structured, and unstructured data, and it seamlessly handles
diverse and complex datasets. MongoDB empowers AI agents to react
dynamically to new information, maintain internal states in real-time,
and continuously learn and evolve.

**Flexible data model:** MongoDB's architecture provides a powerful
foundation for agentic AI systems by offering unparalleled flexibility
in data storage. The document model allows you to store varied data such
as time series logs, agent profiles, and recommendation outputs in a
single, unified format. This flexibility means you don’t have to
redesign your database schema every time your data requirements evolve.
MongoDB's flexible schema enables AI agents to store complex,
hierarchical agent states with nested documents that can dynamically
adapt to changing agent characteristics and interaction contexts.
MongoDB is an ideal choice for maintaining complex agent states across
multiple interactions. It supports versioning and tracking agent
evolution, with the ability to pause and resume agent contexts
seamlessly.

**Vector search:** MongoDB Atlas supports native vector search, enabling
fast and efficient similarity searches on vector embeddings. This is
critical for matching current queries with historical data, thereby
enhancing diagnostic accuracy and providing more relevant
recommendations. Vector search can support pattern recognition and
contextual retrieval that significantly enhance quality and decision
making of AI agents. Vector search enables sophisticated information
retrieval that uses a similarity algorithm to provide results that are
more precise, reducing LLM hallucination. MongoDB can efficiently handle
semantic matching, contextual searches, and multi-dimensional data
analysis, all crucial for AI agents' decision making workflows.

**Scalability, performance and high availability:** MongoDB is designed
to scale horizontally, making it capable of handling large volumes of
real-time data. MongoDB’s seamless horizontal scalability supports
large-scale, distributed AI applications with growing data requirements
for AI workloads through distributed and sharding architecture. With
MongoDB, AI agents can scale horizontally, distribute storage and
computational load, and maintain high availability, ensuring consistent
performance even when volume of data grows exponentially. MongoDB
replica sets provide robust high availability out of the box that
underpins the reliability and resilience of AI agents.

`Coinbase
<https://www.mongodb.com/solutions/customer-case-studies/coinbase>`__,
one of the largest cryptocurrency exchanges in the U.S., has effectively
leveraged MongoDB to significantly enhance its platform scalability
across 700 MongoDB clusters and overcome high volumes of unpredictable
market traffic spikes.

**Time series collections:** MongoDB time series collections are
designed to efficiently ingest large volumes of data with high
performance and scalability. These specialized collections enable AI
agents to track and analyze sequential interactions, learning patterns,
and state changes over time, which is essential for maintaining context,
understanding behavioral patterns, and implementing adaptive
decision-making processes. By offering native time series optimizations
such as automatic data compression, improved storage efficiency, and
fast time-based query responses, MongoDB allows AI agents to maintain
extensive current and historical records of their interactions, learning
iterations, and performance metrics without compromising computational
performance or data integrity.

**Seamless integration:** The seamless integration of MongoDB with
agentic frameworks like LangGraph expedites the development of complex,
stateful AI agentic systems. Through native support for JSON-like
documents, dynamic schema, and powerful indexing capabilities, MongoDB
enables LangGraph-powered AI Agents to maintain sophisticated data and
memory structures, track multi-step reasoning processes, and implement
advanced state management that can persist across sessions and provide
comprehensive auditability of AI decision-making processes.

Building the Solution
---------------------

.. procedure::
   :style: normal

   .. step:: **Include meaningful CSV data for your use case**

      The data should be relevant to your use case and respect the
      following guidelines:

      - The CSV file should contain a header row with column names.
      - The first column must be named “timestamp” and contain
        timestamps in the format YYYY-MM-DDTHH:MM:SSZ, e.g.
        2025-02-19T13:00:00Z.
      - The remaining columns should contain relevant data for your use
        case. 
      - There is no column nor row limit, but for framework
        testing purposes the suggestion is to keep the data size as
        small as possible.

      Sample data:

      .. code-block:: none

         timestamp,gdp,interest_rate,unemployment_rate,vix
         2025-02-19T13:00:00Z,2.5,1.75,3.8,15
         2025-02-19T13:05:00Z,2.7,1.80,3.7,18
         2025-02-19T13:10:00Z,2.6,1.85,3.9,22
         2025-02-19T13:15:00Z,2.4,1.70,4.0,10
         2025-02-19T13:20:00Z,2.3,1.65,4.1,20

      Within the same folder you need to add a queries file. This file
      will contain the queries that will be used to showcase vector
      search capabilities as part of the agentic workflow. The file
      should respect the following guidelines:

      - The CSV file should contain a header row with column names. 
      - The first column must be named “query” and contain the queries. 
      - The second column must be named “recommendation” and contain the
        expected recommendations. 
      - There is no column or row limit, but for framework testing
        purposes the suggestion is to keep the data size as small as
        possible.

      Sample data:

      .. code-block:: none

         query,recommendation
         GDP growth slowing,Consider increasing bond assets to mitigate risks from potential economic slowdown.
         GDP showing strong growth,Increase equity assets to capitalize on favorable investment conditions.
         Interest rates rising,Shift focus to bond assets as higher rates may impact borrowing-sensitive sectors.
         Interest rates falling,Increase real estate assets to take advantage of lower borrowing costs.
         Unemployment rate increasing,Reduce equity assets to account for potential economic weakness and reduced consumer spending.
         Unemployment rate decreasing,Increase equity assets to benefit from improved economic conditions and corporate profits.
         VIX above 20,Reduce equity assets to manage risks associated with high market volatility.
         VIX below 12,Increase equity assets to capitalize on low market volatility and investor confidence.
         VIX within normal range (12-20),Maintain current asset allocation as market conditions are stable.
         Combination of rising interest rates and high VIX,Focus on bond assets to hedge against market volatility and borrowing cost impacts.

   .. step:: **Set up MongoDB database and collections**

      Log in to MongoDB Atlas and create a database named
      “agentic_<YOUR_USE_CASE>,” e.g. “agentic_macro_indicators”. Ensure
      the name is reflected in the environment variables. 
      
      Create the following collections:

      - Agent_profiles (for storing agent profiles): You can import some
        sample data to this collection using this file.
      - Queries (for storing queries): You must import the queries from
        the queries.csv file created in Step 1.

   .. step:: **Configure your agentic worklfow through a JSON config file**

      Go to the config folder and create or update the JSON config.json
      file. The file should contain the following structure:

      .. code-block:: json

         {  
            "CSV_DATA": "data/csv/<YOUR_FILE_NAME>.csv",  
            "MDB_DATABASE_NAME": "<YOUR_MONGODB_DATABASE_NAME>",  
            "MDB_TIMESERIES_COLLECTION": "<YOUR_MONGODB_TIMESERIES_COLLECTION_NAME>",  
            "DEFAULT_TIMESERIES_DATA": [  
               {  
                  "timestamp": "<DEFAULT_TIMESTAMP_IN_YYYY-MM-DDTHH:MM:SSZ>"  
                  // Your default data here, check config_example.json for better understanding  
               }  
            ],  
            "CRITICAL_CONDITIONS": {  
               // Below is an example of a critical condition for GDP growth  
               "gdp": {"threshold": 2.5, "condition": "<", "message": "GDP growth slowing: {value}%"}  
               // Other critical conditions for your use case here, check config_example.json for better understanding  
            },  
            "MDB_TIMESERIES_TIMEFIELD": "<YOUR_TIMESTAMP_FIELD_NAME>",  
            "MDB_TIMESERIES_GRANULARITY": "<YOUR_TIMESERIES_GRANULARITY>",  
            "MDB_EMBEDDINGS_COLLECTION": "queries", // Using "queries" collection name for storing queries  
            "MDB_EMBEDDINGS_COLLECTION_VS_FIELD": "query_embedding", // Using "query_embedding" field for storing embeddings  
            "MDB_VS_INDEX": "<YOUR_MONGODB_DATABASE_NAME>_queries_vs_idx", // Replace <YOUR_MONGODB_DATABASE_NAME> with your MongoDB database name  
            "MDB_HISTORICAL_RECOMMENDATIONS_COLLECTION": "historical_recommendations", // Using "historical_recommendations" collection name for storing recommendations  
            "SIMILAR_QUERIES": [  
               // Below is an example of default similar queries for GDP growth  
               {  
                  "query": "GDP growth slowing",  
                  "recommendation": "Consider increasing bond assets to mitigate risks from potential economic slowdown."  
               }  
               // Other similar queries for your use case here, check config_example.json for better understanding  
               // This ones are going to be used for the vector search tool in case something is not found in the queries collection  
            ],  
            "MDB_CHAT_HISTORY_COLLECTION": "chat_history", // Using "chat_history" collection name for storing chat history  
            "MDB_CHECKPOINTER_COLLECTION": "checkpoints", // Using "checkpoints" collection name for storing checkpoints  
            "MDB_LOGS_COLLECTION": "logs", // Using "logs" collection name for storing logs  
            "MDB_AGENT_PROFILES_COLLECTION": "agent_profiles", // Using "agent_profiles" collection name for storing agent profiles  
            "MDB_AGENT_SESSIONS_COLLECTION": "agent_sessions", // Using "agent_sessions" collection name for storing agent sessions  
            "AGENT_PROFILE_CHOSEN_ID": "<YOUR_AGENT_PROFILE_ID>", // Replace <YOUR_AGENT_PROFILE_ID> with the agent profile ID you want to use, check config_example.json for better understanding  
            // Below is an example default agent profile for Portfolio Advisor  
            "DEFAULT_AGENT_PROFILE": {  
               "agent_id": "DEFAULT",  
               "profile": "Default Agent Profile",  
               "role": "Expert Advisor",  
               "kind_of_data": "Specific Data",  
               "motive": "diagnose the query and provide recommendations",  
               "instructions": "Follow procedures meticulously.",  
               "rules": "Document all steps.",  
               "goals": "Provide actionable recommendations."  
            },  
            "EMBEDDINGS_MODEL_NAME": "Cohere Embed English V3 Model (within AWS Bedrock)", // Describing the embeddings model used for creating the chain of thought  
            "EMBEDDINGS_MODEL_ID": "cohere.embed-english-v3", // Model ID for the embeddings model  
            "CHATCOMPLETIONS_MODEL_NAME": "Anthropic Claude 3 Haiku (within AWS Bedrock)", // Describing the chat completions model used for generating responses  
            "CHATCOMPLETIONS_MODEL_ID": "anthropic.claude-3-haiku-20240307-v1:0", // Model ID for the chat completions model  
            // Below is a sample agent workflow graph that uses the tools defined in the agent_tools.py file  
            // PLEASE BE CAREFUL WHEN MODIFYING THIS GRAPH, CONSIDER THAT THE TOOLS DEFINED IN THE AGENT TOOLS FILE ARE USED HERE AS WELL AS THE IMPORTS  
            "AGENT_WORKFLOW_GRAPH": {  
               "nodes": [  
                  {"id": "reasoning_node", "tool": "agent_tools.generate_chain_of_thought_tool"},  
                  {"id": "data_from_csv", "tool": "agent_tools.get_data_from_csv_tool"},  
                  {"id": "process_data", "tool": "agent_tools.process_data_tool"},  
                  {"id": "embedding_node", "tool": "agent_tools.get_query_embedding_tool"},  
                  {"id": "vector_search", "tool": "agent_tools.vector_search_tool"},  
                  {"id": "process_vector_search", "tool": "agent_tools.process_vector_search_tool"},  
                  {"id": "persistence_node", "tool": "agent_tools.persist_data_tool"},  
                  {"id": "recommendation_node", "tool": "agent_tools.get_llm_recommendation_tool"}  
               ],  
               "edges": [  
                  {"from": "reasoning_node", "to": "data_from_csv"},  
                  {"from": "data_from_csv", "to": "process_data"},  
                  {"from": "process_data", "to": "embedding_node"},  
                  {"from": "embedding_node", "to": "vector_search"},  
                  {"from": "vector_search", "to": "process_vector_search"},  
                  {"from": "process_vector_search", "to": "persistence_node"},  
                  {"from": "persistence_node", "to": "recommendation_node"},  
                  {"from": "recommendation_node", "to": "END"}  
               ],  
               "entry_point": "reasoning_node"  
            }  
         }  

      Once the config file is updated, environment variables need to be
      configured and vector embeddings created. Finally the vector
      search index must be created.

      Check out the `readme
      <https://github.com/mongodb-industry-solutions/agentic-framework/tree/a14ed55e9b4a5628da1798b5e4bbc59d695d43a7?tab=readme-ov-file#step-4-configure-your-agentic-workflow-through-a-json-config-file>`__
      for detailed explanations of all the steps.

      .. figure:: /includes/images/industry-solutions/framework-rapid-logical-architecture.svg
         :figwidth: 1200px
         :alt: Real-time wind turbine diagnosis

         Figure 3. Logical architecture.

Key Learnings
-------------

Agentic AI ushers in an entirely new type of software—one with the
ability to dramatically alter the way we process data to drive richer
and more efficient business data processing outcomes. In order to
leverage agents within business applications, software delivery teams
will need to learn:

- How agents get instructions for reacting within predetermined workflows.
- How agents access and interact with tooling like APIs and databases.
- How agents persist and manage state.

Many of these items can be considered baseline activities and setup for
software delivery teams, regardless of the actual use case, workflow, or
industry context for which an agent, or agentic workflow, will be
deployed.

For high performing software delivery teams, the use of a framework to
abstract common tasks and components has often been employed to speed up
the development and deployment of business software solutions, as well
as decrease the subsequent complexity of maintaining them. This agentic
framework makes it easier for software delivery teams to get started
with building agentic solutions, as well as maintain them, via common,
configurable components and assets.

Working in this framework can not only teach you and your software teams
how to get started with building agentic solutions, it can also help
them build repeatable patterns and components that make it easier to
scale and maintain your business solutions for the long term.

Authors
-------

- Julian Boronat, MongoDB
- Peyman Parsi, MongoDB
- Jeff Needham, MongoDB
- Luca Napoli, MongoDB
- Humza Akthar, MongoDB