===
HA
===

Building the Solution
---------------------

A dataset including the total distance driven in car journeys is loaded
into MongoDB and a daily cron job is run every day at midnight that
summarizes the daily trips and compiles them into a document stored in a
new collection called “CustomerTripDaily.” A monthly cron job is run on
the 25th day of each month, aggregating the daily documents and creating
a new collection called “Customer Trip Monthly.” Every time a new
monthly summary is created, an Atlas function posts the total distance
for the month and baseline premium to Databricks for ML prediction. The
ML prediction is then sent back to MongoDB and added to the “Customer
Trip Monthly” document. As a final step, you can visualize all of your
data with MongoDB Charts.

.. procedure::
   :style: normal

   .. step:: Creating a data processing pipeline with a materialized view

      The data processing pipeline component of this example consists of
      sample data, a daily materialized view, and a monthly materialized
      view. A sample dataset of IoT vehicle telemetry data represents
      the motor vehicle trips taken by customers. It’s loaded into the
      collection named ‘customerTripRaw’ (1). The dataset can be found
      on 
      GitHub and can be loaded via MongoImport or other methods. To create a
      materialized view, a scheduled trigger executes a function that runs an
      aggregation pipeline. This then generates a daily summary of the raw IoT
      data and places it in a materialized view collection named
      ‘customerTripDaily’ (2). Similarly for a monthly materialized view, a
      scheduled trigger executes a function that runs an aggregation pipeline
      that summarizes the information in the ‘customerTripDaily’ collection on
      a monthly basis and places it in a materialized view collection named
      ‘customerTripMonthly’ (3).

      See the following Github repos to create the data processing pipeline:

      -Step 1 `Load the sample data <https://github.com/mongodb-industry-solutions/Digital-Underwriting-Usage-Based-Insurance/blob/main/src/LoadingtheSampleData.md>`__
      -Step 2 `Setup a daily cron job <https://github.com/mongodb-industry-solutions/Digital-Underwriting-Usage-Based-Insurance/blob/main/src/DailyCronJob.md>`__
      -Step 3 `Setup a monthly cron job <https://github.com/mongodb-industry-solutions/Digital-Underwriting-Usage-Based-Insurance/blob/main/src/MonthlyCronJob.md>`_

      .. figure:: /includes/images/industry-solutions/demo-model.svg
         :figwidth: 1200px
         :alt: An illustration shows on how to create a data processing pipeline

        Figure 3: Creating a data processing pipeline

   .. step::  Automating insurance premium calculations with a machine learning model

      The decision-processing component of this example consists of a
      scheduled trigger that collects the necessary data and posts the
      payload to a Databricks ML Flow API endpoint. (The model was
      previously trained using the MongoDB Spark Connector on
      Databricks.) It then waits for the model to respond with a
      calculated premium based on the miles driven by a given customer
      in a month. Then the scheduled trigger updates the
      ‘customerPolicy’ collection to append a new monthly premium
      calculation as a new subdocument within the ‘monthlyPremium’
      array.

      See the following Github repos to create the data processing pipeline:

      -Step 4 `Setup a calculate premium trigger <https://github.com/mongodb-industry-solutions/Digital-Underwriting-Usage-Based-Insurance/blob/main/src/CalculatePremiumTrigger.md>`__
      -Step 5 `Setup the Databricks connection <https://github.com/mongodb-industry-solutions/Digital-Underwriting-Usage-Based-Insurance/blob/main/src/DatabricksConfiguration.md>`__
      -Step 6 `Write the machine learning model prediction to MongoDB <https://github.com/mongodb-industry-solutions/Digital-Underwriting-Usage-Based-Insurance/blob/main/src/Prediction.md>`_

      .. figure:: /includes/images/industry-solutions/demo-model.svg
         :figwidth: 1200px
         :alt: Automating Calculations with Machine Learning Model

        Figure 4: Automating Calculations with Machine Learning Model

   .. step:: Near-real-time insights of insurance premium changes over time

      Once the monthly premium calculations have been appended, it’s
      easy to set up Atlas Charts to visualize your newly calculated
      usage-based premiums. Configure different charts to see how
      premiums have changed over time to discover patterns. 
  
Technologies and Products Used
------------------------------

MongoDB modern, multi-cloud database platform:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- `Atlas Database <https://www.mongodb.com/atlas/database>`__
- `Aggregation Pipelines <https://www.mongodb.com/docs/v7.0/core/aggregation-pipeline/>`__
- `Materialized Views <https://www.mongodb.com/docs/v5.0/core/materialized-views/>`__
- `Time Series <https://www.mongodb.com/time-series>`__
- `MongoDB Spark Connector <https://www.mongodb.com/products/spark-connector>`__
- `Atlas Charts <https://www.mongodb.com/products/charts>`__
- `Atlas App Services <https://www.mongodb.com/atlas/app-services>`__
  -`Triggers <https://www.mongodb.com/docs/atlas/app-services/triggers/>`__
  -`Functions <https://www.mongodb.com/docs/atlas/app-services/functions/>`__

Partner technologies:
~~~~~~~~~~~~~~~~~~~~~

- `Databricks <https://www.mongodb.com/partners/databricks>`__

Key Considerations
------------------

- Building materialized view on time series data: refer to steps 1-3 in
  the GitHub repo.
- Leveraging aggregation pipelines for cron expressions: refer to steps
  2 or 3 in the GitHub repo.
- Serving machine learning models with MongoDB Atlas data: refer to step 4
  in the GitHub repo.
- Writing a machine learning model prediction to an Atlas database:
  refer to step in the GitHub repo.
- Visualizing near-real-time insights of continuously changing model
  results: refer to the Bonus step in the GitHub repo.

Author
------

- Jeff Needham, MongoDB
- Ainhoa Múgica, MongoDB
- Luca Napoli, MongoDB
- Karolina Ruiz Rogelj, MongoDB