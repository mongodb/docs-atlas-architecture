.. _arch-center-is-claim-management:

=====================================================
Claim Management Using LLMs and Vector Search for RAG
=====================================================

.. facet::
   :name: genre
   :values: tutorial

.. meta:: 
   :keywords: RAG, Atlas
   :description: Combine Atlas Vector Search and large language models to streamline the claim adjustment process.
   
.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

Discover how to combine MongoDB Atlas Vector Search and large language models
(LLMs) to streamline the claim adjustment process.

**Use cases:** `Gen AI <https://www.mongodb.com/use-cases/artificial-intelligence>`__, 
`Content Management <https://www.mongodb.com/solutions/use-cases/content-management>`__

**Industries:** `Insurance <https://www.mongodb.com/industries/insurance>`__,
`Finance <https://www.mongodb.com/industries/financial-services>`__,
`Manufacturing and Mobility <https://www.mongodb.com/industries/manufacturing>`__, 
`Retail <https://www.mongodb.com/industries/retail>`__

**Products:** `MongoDB Atlas <http://mongodb.com/atlas>`__, 
`MongoDB Atlas Vector Search <https://www.mongodb.com/products/platform/atlas-vector-search>`__

**Partners:** `LangChain <https://www.mongodb.com/developer/products/mongodb/langchain-vector-search/>`__, 
`FastAPI <https://www.mongodb.com/developer/technologies/fastapi/>`__

Solution Overview
-----------------

One of the biggest challenges for claim adjusters is aggregating information
from disparate systems and diverse data formats. Over the years, insurance
companies have accumulated terabytes of `unstructured data
<https://www.mongodb.com/unstructured-data>`__ in their datastores, but fail to
capitalize on it to uncover business insights, deliver better customer
experiences, and streamline operations.

To benefit these organizations and their customers, MongoDB's solution combines
the power of `Altas Vector Search
<https://www.mongodb.com/products/platform/atlas-vector-search>`__ and a `Large
Language Models (LLMs) <https://www.mongodb.com/basics/large-language-models>`__
in a `retrieval augmented generation (RAG)
<https://www.mongodb.com/basics/retrieval-augmented-generation>`__ system, to
allow organizations to go beyond the limitations of baseline foundational
models. By feeding models propriety data, they are made context aware and can
leverage the full potential of AI to streamline operations.

Reference Architecture
----------------------

With MongoDB
~~~~~~~~~~~~

To provide a unified development experience, documents are stored alongside
their vector embeddings and associated metadata, eliminating the need to
retrieve data elsewhere. This allows users to focus on building their application,
instead of maintaining a separate technology.

Ultimately, the data obtained from MongoDB Vector Search is fed to the LLM as 
context.

The process of the RAG querying flow is as follows:

#. The user writes a prompt in natural language.

#. Voyage AI's embedding model vectorizes the prompt.

#. Atlas Vector Search uses the vectorized prompt to retrieve relevant
   documents.

#. LLM uses both the context and original question to generate an answer.

#. The user receives an answer.

.. figure:: /includes/images/industry-solutions/rag-querying-flow.svg
   :figwidth: 1200px
   :alt: RAG Querying Flow

   Figure 1. RAG querying flow

Data Model Approach
-------------------

The “claim” collection contains documents including a number of fields
related to the claim. In particular, we are interested in the
``claimDescription`` field, which we vectorize and add to the document
as ``claimDescriptionEmbedding``. This embedding is then indexed and
used to retrieve documents associated with the user prompt.

.. code-block:: json
   :emphasize-lines: 4, 10
   :copyable: true

   {
      _id: ObjectId('64d39175e65'),
      customerID: "c113",
      claimDescription: "A motorist driving...",
      damageDescription: "Front-ends of both...",
      lossAmount: 1250,
      photo: "image_65.jpg"
      claimClosedDate: "2024-02-03",
      coverages: Array(2),
      claimDescriptionEmbedding: [0.3, 0.6, ..., 11.2]
   }

Build the Solution
------------------

The instructions to build the demo are included in the README of `this
Github repo <https://github.com/mongodb-industry-solutions/RAG-Insurance>`__.
You’ll be guided through the following steps:

- AWS Account with Bedrock access
- Atlas connection setup
- Dataset download
- LLM configuration options
- Vector Search index creation

Visit the `Atlas Vector Search Quick Start guide
<https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/vector-search-quick-start/?tck=ai_as_web>`__
to try our semantic search tool now.

This `document
<https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-type/#create-an-atlas-vector-search-index>`__
walks you through the creation and configuration of the Vector Search
index. Make sure you follow this structure:

.. code-block:: json
   :copyable: true 

   {
      "fields": [
         { 
            "type": "vector", 
            "path": "claimDescriptionEmbedding",
            "numDimensions": 350, 
            "similarity": "cosine"
         }
      ]
   }

Ultimately, you have to run both the front and the back end. You’ll
access a web UI that allows you to ask questions to the LLM, obtain an
answer, and see the reference documents used as context.

Key Learnings
-------------

- **Text embedding creation:** The embedding generation process can be
  carried out using different models and deployment options. It is
  important to consider privacy and data protection requirements. You
  can deploy a model locally if your data needs to remain on the
  servers. Otherwise, you can call an API and get your vector embeddings
  back, as explained in `this
  <https://www.mongodb.com/docs/atlas/atlas-vector-search/create-embeddings/>`__
  tutorial. You can use `Voyage AI (acquired by MongoDB)
  <https://www.mongodb.com/blog/post/redefining-database-ai-why-mongodb-acquired-voyage-ai>`__
  or open-source models. 

- **Creation of a Vector Search index in Atlas:** It is now possible to
  create indexes for `local deployments
  <https://www.mongodb.com/docs/atlas/cli/stable/atlas-cli-deploy-local/#std-label-atlas-cli-deploy-local-avs>`__.

- **Performing a Vector Search query:** Notably, `Vector Search queries <https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/>`__ 
  have a dedicated operator within MongoDB’s `aggregation pipeline
  <https://www.mongodb.com/docs/manual/aggregation/>`__. This means they
  can be concatenated with other operations, making it extremely
  convenient for developers because they don’t need to learn a different
  language or change context.

- Using LangChain as the framework that glues together MongoDB Atlas
  Vector Search and the LLM, allowing for an easy and fast RAG
  implementation.

Authors
-------

- Luca Napoli, Industry Solutions, MongoDB 
- Jeff Needham, Industry Solutions, MongoDB